---
title: "CNN"
author: "Chengcen Zhou"
output: word_document
---

```{r}

### Construct visual features for training/testing images ###
feature <- function(img_dir, set_name, data_name="data", export=T){
  ### load libraries
  library("EBImage")
  
  n_files <- length(list.files(img_dir))
  
  ### determine img dimensions
  img0 <-  readImage(paste0(img_dir, "img", "_", data_name, "_", set_name, "_", 1, ".jpg"))
  mat1 <- as.matrix(img0)
  n_r <- nrow(img0)
### store vectorized pixel values of images
  dat <- matrix(NA, n_files, n_r) 
  for(i in 1:n_files){
    img <- readImage(paste0(img_dir,  "img", "_", data_name, "_", set_name, "_", i, ".jpg"))
    dat[i,] <- rowMeans(img)
  }
  
  ### output constructed features
  if(export){
    save(dat, file=paste0("../output/feature_", data_name, "_", set_name, ".RData"))
  }
  return(dat)
}

```

```{r}

########################## Baseline ############################################

## Data Preparation
Fea = read.csv("sift_features.csv",as.is = T, header = T)
Fea = t(Fea) #the features as column, data object as row
y = read.csv("../labels.csv",as.is = T) #0 for chicken(not a dog), 1 for dog
Data = cbind(y,Fea)
Data = as.data.frame(Data)
install.packages("gbm")
library("gbm")
######################################################################

## divide train-test(80%-20%)
set.seed(200)
index = sample(1:2000,1600)
Train = Data[index,]
Test = Data[-index,]

######################################################################

## try gbm to build boosted decision stumps

#start the clock
ptm <- proc.time()
## PS. this version of ADABOOST needs response to be {0,1}
a = gbm(V1~.,data = Train,
        distribution = "adaboost",
        n.trees = 100,
        shrinkage=0.1,
        interaction.depth = 1,
        bag.fraction = 1,
        train.fraction = 1,
        cv.folds=5,
        keep.data = TRUE,
        verbose = "CV")
best_iter = gbm.perf(a, method="cv", plot=FALSE)
# Stop the clock
proc.time() - ptm #69 sec
######################################################################
##predict time
ptm <- proc.time()
y_adj = ifelse(y==0,-1,1)
f.predict = predict(a,Train,best_iter)
train_rate = mean(sign(f.predict)!= y_adj[index]) #0.21
t.predict = predict(a,Test,best_iter)
test_rate = mean(sign(t.predict)!= y_adj[-index]) #0.27
proc.time() - ptm #12 sec
######################################################################
```


```{r}
################ CNN Using "mxnet" Package ###############

# Load EBImage library
# require(EBImage)
# rs_df <- data.frame()

# Clean workspace
rm(list=ls())
# Load MXNet
require(mxnet)
# Loading data and set up
#-------------------------------------------------------------------------------
# Load train and test datasets
train = read.csv("/Users/Connie/Desktop/Train.csv",as.is = T, header = T)
test = read.csv("/Users/Connie/Desktop/Test.csv",as.is = T, header = T)
labels <- read.csv("/Users/Connie/Desktop/Train.csv", header = F)
# View(labels)
# View(train)
# View(test)

# Set up train and test datasets
train <- data.matrix(train)
train_x <- t(train[, -1])
train_y <- train[, 1]
train_array <- train_x
# length(train)
#dim(train_array) <- c(798400, 798400, 1, ncol(train_x))

# length(test)
test_x <- t(test[, -1])
test_y <- test[, 1]
test_array <- test_x
#dim(test_array) <- c(499, 499, 1, ncol(test_x))

# Set up the symbolic model
#-------------------------------------------------------------------------------
require(mlbench)
data <- mx.symbol.Variable('data')
# 1st convolutional layer
conv_1 <- mx.symbol.Convolution(data = data, kernel = c(5, 5), num_filter = 20)
tanh_1 <- mx.symbol.Activation(data = conv_1, act_type = "tanh")
pool_1 <- mx.symbol.Pooling(data = tanh_1, pool_type = "max", kernel = c(2, 2), stride = c(2, 2))
# 2nd convolutional layer
conv_2 <- mx.symbol.Convolution(data = pool_1, kernel = c(5, 5), num_filter = 50)
tanh_2 <- mx.symbol.Activation(data = conv_2, act_type = "tanh")
pool_2 <- mx.symbol.Pooling(data=tanh_2, pool_type = "max", kernel = c(2, 2), stride = c(2, 2))
# 1st fully connected layer
flatten <- mx.symbol.Flatten(data = pool_2)
fc_1 <- mx.symbol.FullyConnected(data = flatten, num_hidden = 500)
tanh_3 <- mx.symbol.Activation(data = fc_1, act_type = "tanh")
# 2nd fully connected layer
fc_2 <- mx.symbol.FullyConnected(data = tanh_3, num_hidden = 40)
# Output. Softmax output since we'd like to get some probabilities.
NN_model <- mx.symbol.SoftmaxOutput(data = fc_2)
# Pre-training set up
#-------------------------------------------------------------------------------
# Set seed for reproducibility
mx.set.seed(100)
# Device used. CPU in my case.
devices <- mx.cpu()
# Training
#-------------------------------------------------------------------------------
# Train the model
model <- mx.model.FeedForward.create(NN_model,
                                     X = train_array,
                                     y = train_y,
                                     ctx = devices,
                                     num.round = 499,
                                     array.batch.size = 40,
                                     learning.rate = 0.01,
                                     momentum = 0.9,
                                     eval.metric = mx.metric.accuracy,
                                     epoch.end.callback = mx.callback.log.train.metric(100))

# Testing
#-------------------------------------------------------------------------------
# Predict labels
predicted <- predict(model, test_array)
# Assign labels
predicted_labels <- max.col(t(predicted)) - 1
# Get accuracy
sum(diag(table(test[, 1], predicted_labels)))/40

```


```{r}

```


