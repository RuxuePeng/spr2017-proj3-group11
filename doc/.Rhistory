Test = Data[-index,]
######################################################################
#start the clock
ptm <- proc.time()
## PS. this version of ADABOOST needs response to be {0,1}
b = gbm(y~.,data = Train,
distribution = "adaboost",
n.trees = 1200,
shrinkage=0.01,
interaction.depth = 1, # stump
bag.fraction = 0.8,
train.fraction = 1,
cv.folds=5,
keep.data = TRUE,
verbose = "CV")
best_iter = gbm.perf(b, method="cv", plot=FALSE)
# Stop the clock
proc.time() - ptm #228 sec
######################################################################
##predict time
ptm <- proc.time()
y_adj = ifelse(unlist(y)==0,-1,1)
f.predict = predict(b,Train,best_iter)
train_rate = mean(sign(f.predict)!= y_adj[index]) #0.195
t.predict = predict(b,Test,best_iter)
test_rate = mean(sign(t.predict)!= y_adj[-index]) #0.255
proc.time() - ptm # 12 sec
######################################################################
baseline_train = 1-train_rate
baseline_test = 1 - test_rate
cat(paste("Baseline Train Accuracy: ", baseline_train, "\n", "Baseline Test Accuracy", baseline_test))
rm(list=ls())
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
library("glmnet")
library(RSNNS)
library(e1071)
source("../lib/feature_selection.r")
source("../lib/test.R")
source("../lib/train.R")
rm(list=ls())
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
library("glmnet")
library(RSNNS)
library(e1071)
source("../lib/feature_selection.r")
source("../lib/test.R")
source("../lib/train.R")
rm(list=ls())
rm(list=ls())
rm(list=ls())
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
library("glmnet")
library(RSNNS)
library(e1071)
source("../lib/feature_selection.r")
source("../lib/test.R")
source("../lib/train.R")
rm(list=ls())
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
library("glmnet")
library(RSNNS)
library(e1071)
source("../lib/feature_selection.r")
source("../lib/test.R")
source("../lib/train.R")
######################################################################
## Data Preparation ##
Fea = read.csv("../data/sift_features/sift_features.csv",as.is = T, header = T)
Fea = t(Fea) #the features as column, data object as row
# for randomForest to use classification unstead of regression,
# we need to factorize y
y = read.csv("../data/labels.csv",as.is = T) #0 for chicken(not a dog), 1 for dog
######################################################################
Data = cbind(y,Fea)
Data = as.data.frame(Data)
# change colnames
names = c("y",paste0("Fea",1:5000))
colnames(Data) = names
## divide train-test(80%-20%)
set.seed(200)
index = sample(1:2000,1600)
Train = Data[index,]
Test = Data[-index,]
######################################################################
#start the clock
ptm <- proc.time()
## PS. this version of ADABOOST needs response to be {0,1}
b = gbm(y~.,data = Train,
distribution = "adaboost",
n.trees = 1200,
shrinkage=0.01,
interaction.depth = 1, # stump
bag.fraction = 0.8,
train.fraction = 1,
cv.folds=5,
keep.data = TRUE,
verbose = "CV")
best_iter = gbm.perf(b, method="cv", plot=FALSE)
# Stop the clock
proc.time() - ptm #228 sec
######################################################################
##predict time
ptm <- proc.time()
y_adj = ifelse(unlist(y)==0,-1,1)
f.predict = predict(b,Train,best_iter)
train_rate = mean(sign(f.predict)!= y_adj[index]) #0.195
t.predict = predict(b,Test,best_iter)
test_rate = mean(sign(t.predict)!= y_adj[-index]) #0.255
proc.time() - ptm # 12 sec
######################################################################
baseline_train = 1-train_rate
baseline_test = 1 - test_rate
cat(paste("Baseline Train Accuracy: ", baseline_train, "\n", "Baseline Test Accuracy", baseline_test))
nzv_cols <- nearZeroVar(Data)
library(caret)
nzv_cols <- nearZeroVar(Data)
if(length(nzv_cols) > 0) Data <- Data[, -nzv_cols]
Data = cbind(y,Fea)
Data = as.data.frame(Data)
nzv_cols <- nearZeroVar(Data)
if(length(nzv_cols) > 0) Data <- Data[, -nzv_cols]
# change colnames
names = c("y",paste0("Fea",1:5000))
colnames(Data) = names
names = c("y",paste0("Fea",1:length(Data)))
colnames(Data) = names
nzv_cols <- nearZeroVar(Fea)
if(length(nzv_cols) > 0) Fea <- Fea[, -nzv_cols]
Data = cbind(y,Fea)
Data = as.data.frame(Data)
# change colnames
names = c("y",paste0("Fea",1:length(Data)))
colnames(Data) = names
View(Data)
View(Fea)
names = c("y",paste0("Fea",1:length(Data)-1))
colnames(Data) = names
names = c("y",paste0("Fea",(1:length(Data)-1)))
colnames(Data) = names
length(Data)
colnames(Data)
names = c("y",paste0("Fea",1:(length(Data)-1))
colnames(Data) = names
names = c("y",paste0("Fea",1:(length(Data)-1)))
colnames(Data) = names
View(Data)
## divide train-test(80%-20%)
set.seed(200)
index = sample(1:2000,1600)
Train = Data[index,]
Test = Data[-index,]
#start the clock
ptm <- proc.time()
## PS. this version of ADABOOST needs response to be {0,1}
b = gbm(y~.,data = Train,
distribution = "adaboost",
n.trees = 1200,
shrinkage=0.01,
interaction.depth = 1, # stump
bag.fraction = 0.8,
train.fraction = 1,
cv.folds=5,
keep.data = TRUE,
verbose = "CV")
best_iter = gbm.perf(b, method="cv", plot=FALSE)
# Stop the clock
proc.time() - ptm #228 sec
######################################################################
##predict time
ptm <- proc.time()
y_adj = ifelse(unlist(y)==0,-1,1)
f.predict = predict(b,Train,best_iter)
train_rate = mean(sign(f.predict)!= y_adj[index]) #0.195
t.predict = predict(b,Test,best_iter)
test_rate = mean(sign(t.predict)!= y_adj[-index]) #0.255
proc.time() - ptm # 12 sec
######################################################################
baseline_train = 1-train_rate
baseline_test = 1 - test_rate
cat(paste("Baseline Train Accuracy: ", baseline_train, "\n", "Baseline Test Accuracy", baseline_test))
feature_selection_mlp(Fea,y)
rm(list=ls())
rm(list=ls())
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
library("glmnet")
library(RSNNS)
library(e1071)
library(caret)
source("../lib/feature_selection.r")
source("../lib/test.R")
source("../lib/train.R")
######################################################################
## Data Preparation ##
Fea = read.csv("../data/sift_features/sift_features.csv",as.is = T, header = T)
Fea = t(Fea) #the features as column, data object as row
# for randomForest to use classification unstead of regression,
# we need to factorize y
y = read.csv("../data/labels.csv",as.is = T) #0 for chicken(not a dog), 1 for dog
######################################################################
nzv_cols <- nearZeroVar(Fea)
if(length(nzv_cols) > 0) baseline_features <- Fea[, -nzv_cols]
Data = cbind(y,baseline_features)
Data = as.data.frame(Data)
# change colnames
names = c("y",paste0("Fea",1:(length(Data)-1)))
colnames(Data) = names
## divide train-test(80%-20%)
set.seed(200)
index = sample(1:2000,1600)
Train = Data[index,]
Test = Data[-index,]
######################################################################
#start the clock
ptm <- proc.time()
## PS. this version of ADABOOST needs response to be {0,1}
b = gbm(y~.,data = Train,
distribution = "adaboost",
n.trees = 1200,
shrinkage=0.01,
interaction.depth = 1, # stump
bag.fraction = 0.8,
train.fraction = 1,
cv.folds=5,
keep.data = TRUE,
verbose = "CV")
best_iter = gbm.perf(b, method="cv", plot=FALSE)
# Stop the clock
proc.time() - ptm #228 sec
######################################################################
##predict time
ptm <- proc.time()
y_adj = ifelse(unlist(y)==0,-1,1)
f.predict = predict(b,Train,best_iter)
train_rate = mean(sign(f.predict)!= y_adj[index]) #0.195
t.predict = predict(b,Test,best_iter)
test_rate = mean(sign(t.predict)!= y_adj[-index]) #0.255
proc.time() - ptm # 12 sec
######################################################################
baseline_train = 1-train_rate
baseline_test = 1 - test_rate
cat(paste("Baseline Train Accuracy: ", baseline_train, "\n", "Baseline Test Accuracy", baseline_test))
feature_selection_mlp(Fea,y)
feature_selection_svm(Fea,y)
Train_mlp=read.csv("../data/Train_nn.csv")
Test_mlp=read.csv("../data/Test_nn.csv")
set.seed(129)
result_mlp=mlp_tune(Train_mlp, Test_mlp)
view(result_mlp)
view(result_mlp)
View(result_mlp)
View(result_mlp)
MLP=mlp_train(size=27, maxit=17, Train = Train_mlp)
pred_mlp=mlp_test(MLP, Test = Test_mlp, Train = Train_mlp)
cor(result_mlp)
plot(result_mlp$size, result_mlp$testAcc)
plot(result_mlp$iteration, result_mlp$testAcc)
cor(result_mlp)
plot(result_mlp$testAcc, result_mlp$size)
plot(result_mlp$testAcc, result_mlp$iteration)
cor(result_mlp)
plot(result_mlp$size, result_mlp$testAcc)
plot(result_mlp$iteration, result_mlp$testAcc)
Train_svm=read.csv("../data/Train_svm.csv")
Test_svm=read.csv("../data/Test_svm.csv")
result_svm=svm_tune(Train_svm, Test_svm)
View(result_svm)
cor(result_mlp)
plot(result_svm$cost, result_svm$testAcc)
cor(result_svm)
plot(result_svm$cost, result_svm$testAcc)
source("../lib/train.R")
SVM= svm_train(cost=0.0015, Train = Train_svm)
pred_svm = svm_test(SVM, Test = Test_svm, Train = Train_svm)
Train_svm=read.csv("../data/Train_svm.csv")
Test_svm=read.csv("../data/Test_svm.csv")
result_svm=svm_tune(Train_svm, Test_svm)
View(result_svm)
View(result_svm)
SVM= svm_train(cost=0.0015, Train = Train_svm)
pred_svm = svm_test(SVM, Test = Test_svm, Train = Train_svm)
cor(result_svm)
plot(result_svm$cost, result_svm$testAcc)
View(result_svm)
etm = proc.time()-stm
stm=proc.time()
x=3
etm=proc.time()-stm
etm
stm
stm
stm=proc.time()
stm
etm$1
etm$[1]
etm[1]
etm[2]
etm[3]
etm[4]
etm[5]
etm[6]
ptm03
ptm[3]
ptm[2]
ptm[1]
ptm
cat("Time for making prediction=", tm_test[1], "s \n")
rm(list=ls())
rm(list=ls())
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
library("glmnet")
library(RSNNS)
library(e1071)
library(caret)
source("../lib/feature_selection.r")
source("../lib/test.R")
source("../lib/train.R")
######################################################################
## Data Preparation ##
Fea = read.csv("../data/sift_features/sift_features.csv",as.is = T, header = T)
Fea = t(Fea) #the features as column, data object as row
# for randomForest to use classification unstead of regression,
# we need to factorize y
y = read.csv("../data/labels.csv",as.is = T) #0 for chicken(not a dog), 1 for dog
######################################################################
ptm = proc.time()
nzv_cols <- nearZeroVar(Fea)
if(length(nzv_cols) > 0) baseline_features <- Fea[, -nzv_cols]
Data = cbind(y,baseline_features)
Data = as.data.frame(Data)
# change colnames
names = c("y",paste0("Fea",1:(length(Data)-1)))
colnames(Data) = names
## divide train-test(80%-20%)
set.seed(200)
index = sample(1:2000,1600)
Train = Data[index,]
Test = Data[-index,]
baseline_features_time = proc.time() ptm
baseline_features_time = proc.time() -ptm
cat("Time for constructing baseline features = ", baseline_features_time[3]/60, "minutes \n")
rm(list=ls())
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
library("glmnet")
library(RSNNS)
library(e1071)
library(caret)
source("../lib/feature_selection.r")
source("../lib/test.R")
source("../lib/train.R")
######################################################################
## Data Preparation ##
Fea = read.csv("../data/sift_features/sift_features.csv",as.is = T, header = T)
Fea = t(Fea) #the features as column, data object as row
# for randomForest to use classification unstead of regression,
# we need to factorize y
y = read.csv("../data/labels.csv",as.is = T) #0 for chicken(not a dog), 1 for dog
######################################################################
ptm = proc.time()
nzv_cols <- nearZeroVar(Fea)
if(length(nzv_cols) > 0) baseline_features <- Fea[, -nzv_cols]
Data = cbind(y,baseline_features)
Data = as.data.frame(Data)
# change colnames
names = c("y",paste0("Fea",1:(length(Data)-1)))
colnames(Data) = names
## divide train-test(80%-20%)
set.seed(200)
index = sample(1:2000,1600)
Train = Data[index,]
Test = Data[-index,]
baseline_features_time = proc.time() -ptm
######################################################################
#start the clock
ptm <- proc.time()
## PS. this version of ADABOOST needs response to be {0,1}
b = gbm(y~.,data = Train,
distribution = "adaboost",
n.trees = 1200,
shrinkage=0.01,
interaction.depth = 1, # stump
bag.fraction = 0.8,
train.fraction = 1,
cv.folds=5,
keep.data = TRUE,
verbose = "CV")
best_iter = gbm.perf(b, method="cv", plot=FALSE)
# Stop the clock
baseline_training_time = proc.time() - ptm #228 sec
######################################################################
##predict time
ptm <- proc.time()
y_adj = ifelse(unlist(y)==0,-1,1)
f.predict = predict(b,Train,best_iter)
train_rate = mean(sign(f.predict)!= y_adj[index]) #0.195
t.predict = predict(b,Test,best_iter)
test_rate = mean(sign(t.predict)!= y_adj[-index]) #0.255
baseline_prediction_time = proc.time() - ptm # 12 sec
######################################################################
baseline_train = 1-train_rate
baseline_test = 1 - test_rate
cat(paste("Baseline Train Accuracy: ", baseline_train, "\n", "Baseline Test Accuracy", baseline_test))
ptm = proc.time()
feature_selection_mlp(Fea,y)
feature_selection_mlp_time = proc.time() - ptm
ptm = proc.time()
feature_selection_svm(Fea,y)
feature_selection_svm_time = proc.time() - ptm
Train_mlp=read.csv("../data/Train_nn.csv")
Test_mlp=read.csv("../data/Test_nn.csv")
set.seed(129)
ptm = proc.time()
result_mlp=mlp_tune(Train_mlp, Test_mlp)
mlp_tune_time = proc.time() - ptm
View(result_mlp)
ptm = proc.time()
MLP=mlp_train(size=27, maxit=17, Train = Train_mlp)
mlp_train_time = proc.time() - ptm
ptm = proc.time()
pred_mlp=mlp_test(MLP, Test = Test_mlp, Train = Train_mlp)
mlp_predict_time = proc.time() - ptm
cor(result_mlp)
plot(result_mlp$size, result_mlp$testAcc)
plot(result_mlp$iteration, result_mlp$testAcc)
Train_svm=read.csv("../data/Train_svm.csv")
Test_svm=read.csv("../data/Test_svm.csv")
ptm = proc.time()
result_svm=svm_tune(Train_svm, Test_svm)
svm_tune_time = ptm = proc.time() - ptm
View(result_svm)
ptm = proc.time()
SVM= svm_train(cost=0.0015, Train = Train_svm)
svm_train_time = ptm = proc.time() - ptm
ptm = proc.time()
pred_svm = svm_test(SVM, Test = Test_svm, Train = Train_svm)
svm_predict_time = ptm = proc.time() - ptm
cor(result_svm)
plot(result_svm$cost, result_svm$testAcc)
cat("Time for constructing baseline features = ", baseline_features_time[3]/60, "minutes \n")
cat("Time for training baseline = ", baseline_training_time[3]/60, "minutes \n")
cat("Time for constructing MLP features = ", feature_selection_mlp_time[3]/60, "minutes \n")
cat("Time for tuning MLP model = ", mlp_tune_time[3]/60, "minutes \n")
cat("Time for training MLP model = ", mlp_train_time[3]/60, "minutes \n")
cat("Time for predicting with MLP model = ", mlp_predict_time[3]/60, "minutes \n")
cat("Time for constructing SVM features = ", feature_selection_svm_time[3]/60, "minutes \n")
cat("Time for tuning SVM model = ", svm_tune_time[3]/60, "minutes \n")
cat("Time for training SVM model = ", svm_training_time[3]/60, "minutes \n")
cat("Time for constructing baseline features = ", baseline_features_time[3]/60, "minutes \n")
cat("Time for training baseline = ", baseline_training_time[3]/60, "minutes \n")
cat("Time for constructing MLP features = ", feature_selection_mlp_time[3]/60, "minutes \n")
cat("Time for tuning MLP model = ", mlp_tune_time[3]/60, "minutes \n")
cat("Time for training MLP model = ", mlp_train_time[3]/60, "minutes \n")
cat("Time for predicting with MLP model = ", mlp_predict_time[3]/60, "minutes \n")
cat("Time for constructing SVM features = ", feature_selection_svm_time[3]/60, "minutes \n")
cat("Time for tuning SVM model = ", svm_tune_time[3]/60, "minutes \n")
cat("Time for training SVM model = ", svm_train_time[3]/60, "minutes \n")
cat("Time for predicting with SVM model = ", svm_predict_time[3]/60, "minutes \n")
